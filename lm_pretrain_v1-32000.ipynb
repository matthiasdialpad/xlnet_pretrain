{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  2 01:45:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    43W / 300W |    417MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1712      C   /opt/conda/bin/python                        407MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Check that tensorflow recognizes the GPU\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Cannot recognize GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_filter(df):\n",
    "    df[\"final_utterance\"] = df[\"final_utterance\"].str.lower()\n",
    "\n",
    "    regex_patterns={\n",
    "      # replace \"uh.,\" \"um.,\" \"ah.,\" \"mm.,\", \"oh.,!?\" with blank but we leave uh-hum/mm-hmm in there\n",
    "      r'(\\buh(\\,|\\.|\\s|\\?))': ' ',\n",
    "      r'(\\bum(\\,|\\.|\\s|\\?))': ' ',\n",
    "      r'(\\bah\\W)': ' ',\n",
    "      r'(\\bmm(\\,|\\.|\\s|\\!|\\?))': ' ',\n",
    "      r'(\\boh(\\,|\\.|\\s|\\!|\\?))': ' ',\n",
    "\n",
    "      r'(\\s*(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})?[-. )]*(\\d{3})[-. ]*(\\d{4})(?: *x(\\d+))?\\s*)': ' ', # remove 10 digit and 7 digit numbers  e.g., 456-123-0000 and 123-0000\n",
    "      r'(\\d{3,})': '', # remove any numbers that has length 3 or more \n",
    "      r'\\b(\\w+)((?:\\W+?)(\\s+)?\\1\\b)+': r'\\1', # replaces duplicate words with a single instance of that word. \"yup, yup thank thank you. bye-bye\" -> \"yup thank you. bye\" \n",
    "      r'([a-zA-Z0-9._-]+(\\s+)?@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)': '[email]',\n",
    "      r'(\\bok\\b)': 'okay',  #replaces \"ok\" with \"okay\"\n",
    "      r'(ma\\'am)': 'madam', #replaces ma'am with madam \n",
    "      r'(\\s{2,})': ' '#replaces 2 or more whitespaces with a single one\n",
    "    }\n",
    "\n",
    "    df['final_utterance'].replace(regex=regex_patterns, inplace=True)\n",
    "    df['final_utterance'] = df['final_utterance'].str.strip() \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/xlnet_pretrain'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call_id</th>\n",
       "      <th>time_end</th>\n",
       "      <th>final_utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5666280148631552</td>\n",
       "      <td>444.508</td>\n",
       "      <td>yeah try and have them click on option number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5666280148631552</td>\n",
       "      <td>431.395</td>\n",
       "      <td>okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5666280148631552</td>\n",
       "      <td>456.685</td>\n",
       "      <td>okay, and then can you just double check the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5666280148631552</td>\n",
       "      <td>462.058</td>\n",
       "      <td>mm-hmm so yup hurting good pr pse tendered.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5666280148631552</td>\n",
       "      <td>466.348</td>\n",
       "      <td>your peer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            call_id  time_end  \\\n",
       "0  5666280148631552   444.508   \n",
       "1  5666280148631552   431.395   \n",
       "2  5666280148631552   456.685   \n",
       "3  5666280148631552   462.058   \n",
       "4  5666280148631552   466.348   \n",
       "\n",
       "                                     final_utterance  \n",
       "0  yeah try and have them click on option number ...  \n",
       "1                                              okay.  \n",
       "2  okay, and then can you just double check the o...  \n",
       "3        mm-hmm so yup hurting good pr pse tendered.  \n",
       "4                                         your peer.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id = 'talkiq-data'\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT call_id, time_end, final_utterance\n",
    "FROM `talkiq-data.ai_research.dialpad_xlnet_lm_pretrain`\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(query).to_dataframe() \n",
    "df = regex_filter(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('train_sentencepiece.txt', df.final_utterance.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data for pretraining an XLNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# df_temp = df.groupby(['call_id'])[['time_end']].agg(np.max).reset_index().rename(columns={'time_end':'max_timeend'})\n",
    "# df_processed = df.join(df_temp.set_index('call_id'), on='call_id')\n",
    "# df_processed.loc[(df_processed['time_end'] == df_processed['max_timeend']), ['final_utterance']] = df_processed.final_utterance + \" \\n\"\n",
    "\n",
    "call_ids=df.call_id\n",
    "unequal = call_ids.index[call_ids.shift(-1) != call_ids ]\n",
    "df['final_utterance'].loc[unequal] += ' \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('lm_pretrain.txt', df.final_utterance.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python xlnet/data_utils.py --use_tpu=False --bsz_per_host=4 --num_core_per_host=1 --uncased=True --seq_len=512 --reuse_len=256 --input_glob=\"lm_pretrain.txt\" --save_dir='./dialpad_xlnet_4/' --sp_path='./dialpad_xlnet_4/spiece.model' --mask_alpha=6 --mask_beta=1 --num_predict=85 --bi_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "INFO:tensorflow:Use glob: lm_pretrain.txt\n",
      "INFO:tensorflow:Find 1 files: ['./lm_pretrain.txt']\n",
      "INFO:tensorflow:Task 0 process 1 files: ['./lm_pretrain.txt']\n",
      "INFO:tensorflow:Processing ./lm_pretrain.txt\n",
      "INFO:tensorflow:Loading line 0\n",
      "INFO:tensorflow:Loading line 100000\n",
      "INFO:tensorflow:Loading line 200000\n",
      "INFO:tensorflow:Loading line 300000\n",
      "INFO:tensorflow:Loading line 400000\n",
      "INFO:tensorflow:Loading line 500000\n",
      "INFO:tensorflow:Loading line 600000\n",
      "INFO:tensorflow:Loading line 700000\n",
      "INFO:tensorflow:Loading line 800000\n",
      "INFO:tensorflow:Loading line 900000\n",
      "INFO:tensorflow:Loading line 1000000\n",
      "INFO:tensorflow:Loading line 1100000\n",
      "INFO:tensorflow:Finish with line 1186477\n",
      "INFO:tensorflow:[Task 0] Total number line: 1186477\n",
      "INFO:tensorflow:Using perm indices [0] for pass 0\n",
      "INFO:tensorflow:Raw data shape (4, 12057312).\n",
      "INFO:tensorflow:Start writing ./dialpad_xlnet_32000/tfrecords/train-0-0.bsz-4.vocabsz-32000.seqlen-512.reuse-256.uncased.bi.alpha-6.beta-1.fnp-85.tfrecords.\n",
      "INFO:tensorflow:Processing batch 0\n",
      "INFO:tensorflow:Processing batch 500\n",
      "INFO:tensorflow:Processing batch 1000\n",
      "INFO:tensorflow:Processing batch 1500\n",
      "INFO:tensorflow:Processing batch 2000\n",
      "INFO:tensorflow:Processing batch 2500\n",
      "INFO:tensorflow:Processing batch 3000\n",
      "INFO:tensorflow:Processing batch 3500\n",
      "INFO:tensorflow:Processing batch 4000\n",
      "INFO:tensorflow:Processing batch 4500\n",
      "INFO:tensorflow:Processing batch 5000\n",
      "INFO:tensorflow:Processing batch 5500\n",
      "INFO:tensorflow:Processing batch 6000\n",
      "INFO:tensorflow:Processing batch 6500\n",
      "INFO:tensorflow:Processing batch 7000\n",
      "INFO:tensorflow:Processing batch 7500\n",
      "INFO:tensorflow:Processing batch 8000\n",
      "INFO:tensorflow:Processing batch 8500\n",
      "INFO:tensorflow:Processing batch 9000\n",
      "INFO:tensorflow:Processing batch 9500\n",
      "INFO:tensorflow:Processing batch 10000\n",
      "INFO:tensorflow:Processing batch 10500\n",
      "INFO:tensorflow:Processing batch 11000\n",
      "INFO:tensorflow:Processing batch 11500\n",
      "INFO:tensorflow:Processing batch 12000\n",
      "INFO:tensorflow:Processing batch 12500\n",
      "INFO:tensorflow:Processing batch 13000\n",
      "INFO:tensorflow:Processing batch 13500\n",
      "INFO:tensorflow:Processing batch 14000\n",
      "INFO:tensorflow:Processing batch 14500\n",
      "INFO:tensorflow:Processing batch 15000\n",
      "INFO:tensorflow:Processing batch 15500\n",
      "INFO:tensorflow:Processing batch 16000\n",
      "INFO:tensorflow:Processing batch 16500\n",
      "INFO:tensorflow:Processing batch 17000\n",
      "INFO:tensorflow:Processing batch 17500\n",
      "INFO:tensorflow:Processing batch 18000\n",
      "INFO:tensorflow:Processing batch 18500\n",
      "INFO:tensorflow:Processing batch 19000\n",
      "INFO:tensorflow:Processing batch 19500\n",
      "INFO:tensorflow:Processing batch 20000\n",
      "INFO:tensorflow:Processing batch 20500\n",
      "INFO:tensorflow:Processing batch 21000\n",
      "INFO:tensorflow:Processing batch 21500\n",
      "INFO:tensorflow:Processing batch 22000\n",
      "INFO:tensorflow:Processing batch 22500\n",
      "INFO:tensorflow:Processing batch 23000\n",
      "INFO:tensorflow:Processing batch 23500\n",
      "INFO:tensorflow:Processing batch 24000\n",
      "INFO:tensorflow:Processing batch 24500\n",
      "INFO:tensorflow:Processing batch 25000\n",
      "INFO:tensorflow:Processing batch 25500\n",
      "INFO:tensorflow:Processing batch 26000\n",
      "INFO:tensorflow:Processing batch 26500\n",
      "INFO:tensorflow:Processing batch 27000\n",
      "INFO:tensorflow:Processing batch 27500\n",
      "INFO:tensorflow:Processing batch 28000\n",
      "INFO:tensorflow:Processing batch 28500\n",
      "INFO:tensorflow:Processing batch 29000\n",
      "INFO:tensorflow:Processing batch 29500\n",
      "INFO:tensorflow:Processing batch 30000\n",
      "INFO:tensorflow:Processing batch 30500\n",
      "INFO:tensorflow:Processing batch 31000\n",
      "INFO:tensorflow:Processing batch 31500\n",
      "INFO:tensorflow:Processing batch 32000\n",
      "INFO:tensorflow:Processing batch 32500\n",
      "INFO:tensorflow:Processing batch 33000\n",
      "INFO:tensorflow:Processing batch 33500\n",
      "INFO:tensorflow:Processing batch 34000\n",
      "INFO:tensorflow:Processing batch 34500\n",
      "INFO:tensorflow:Processing batch 35000\n",
      "INFO:tensorflow:Processing batch 35500\n",
      "INFO:tensorflow:Processing batch 36000\n",
      "INFO:tensorflow:Processing batch 36500\n",
      "INFO:tensorflow:Processing batch 37000\n",
      "INFO:tensorflow:Processing batch 37500\n",
      "INFO:tensorflow:Processing batch 38000\n",
      "INFO:tensorflow:Processing batch 38500\n",
      "INFO:tensorflow:Processing batch 39000\n",
      "INFO:tensorflow:Processing batch 39500\n",
      "INFO:tensorflow:Processing batch 40000\n",
      "INFO:tensorflow:Processing batch 40500\n",
      "INFO:tensorflow:Processing batch 41000\n",
      "INFO:tensorflow:Processing batch 41500\n",
      "INFO:tensorflow:Processing batch 42000\n",
      "INFO:tensorflow:Processing batch 42500\n",
      "INFO:tensorflow:Processing batch 43000\n",
      "INFO:tensorflow:Processing batch 43500\n",
      "INFO:tensorflow:Processing batch 44000\n",
      "INFO:tensorflow:Processing batch 44500\n",
      "INFO:tensorflow:Processing batch 45000\n",
      "INFO:tensorflow:Processing batch 45500\n",
      "INFO:tensorflow:Processing batch 46000\n",
      "INFO:tensorflow:Processing batch 46500\n",
      "INFO:tensorflow:Processing batch 47000\n",
      "INFO:tensorflow:Done writing ./dialpad_xlnet_32000/tfrecords/train-0-0.bsz-4.vocabsz-32000.seqlen-512.reuse-256.uncased.bi.alpha-6.beta-1.fnp-85.tfrecords. Num of batches: 47097\n"
     ]
    }
   ],
   "source": [
    "#bi_data = True\n",
    "#spiece.model in this folder is the original one used by the authors \n",
    "!python xlnet/data_utils.py --use_tpu=False --vocab_size=32000 --bsz_per_host=4 --num_core_per_host=1 --uncased=True --seq_len=512 --reuse_len=256 --input_glob=\"lm_pretrain.txt\" --save_dir='./dialpad_xlnet_32000/' --sp_path='./dialpad_xlnet_32000/spiece.model' --mask_alpha=6 --mask_beta=1 --num_predict=85 --bi_data=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet Model Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "INFO:tensorflow:n_token 32000\n",
      "INFO:tensorflow:Use the following tfrecord dirs: ['./dialpad_xlnet_32000/tfrecords/']\n",
      "INFO:tensorflow:[0] Record glob: ./dialpad_xlnet_32000/tfrecords/record_info-train-*.bsz-4.vocabsz-32000.seqlen-512.reuse-256.uncased.bi.alpha-6.beta-1.fnp-85.json\n",
      "INFO:tensorflow:[0] Num of record info path: 1\n",
      "INFO:tensorflow:[Dir 0] Number of chosen batches: 47097\n",
      "INFO:tensorflow:[Dir 0] Number of chosen files: 1\n",
      "INFO:tensorflow:['./dialpad_xlnet_32000/tfrecords/train-0-0.bsz-4.vocabsz-32000.seqlen-512.reuse-256.uncased.bi.alpha-6.beta-1.fnp-85.tfrecords']\n",
      "INFO:tensorflow:Total number of batches: 47097\n",
      "INFO:tensorflow:Total number of files: 1\n",
      "INFO:tensorflow:['./dialpad_xlnet_32000/tfrecords/train-0-0.bsz-4.vocabsz-32000.seqlen-512.reuse-256.uncased.bi.alpha-6.beta-1.fnp-85.tfrecords']\n",
      "INFO:tensorflow:num of batches 47097\n",
      "INFO:tensorflow:Host 0 handles 1 files\n",
      "INFO:tensorflow:label: Tensor(\"Cast_6:0\", shape=(1,), dtype=int32)\n",
      "INFO:tensorflow:seg_id: Tensor(\"Cast_7:0\", shape=(512,), dtype=int32)\n",
      "INFO:tensorflow:target_mapping: Tensor(\"Reshape_4:0\", shape=(85, 512), dtype=float32)\n",
      "INFO:tensorflow:target: Tensor(\"Cast_8:0\", shape=(85,), dtype=int32)\n",
      "INFO:tensorflow:target_mask: Tensor(\"Reshape_6:0\", shape=(85,), dtype=float32)\n",
      "INFO:tensorflow:perm_mask: Tensor(\"Reshape_7:0\", shape=(512, 512), dtype=float32)\n",
      "INFO:tensorflow:input_k: Tensor(\"Cast_9:0\", shape=(512,), dtype=int32)\n",
      "INFO:tensorflow:input_q: Tensor(\"Reshape_9:0\", shape=(512,), dtype=float32)\n",
      "INFO:tensorflow:memory input [<tf.Tensor 'Placeholder:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_1:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_4:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_5:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_6:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_7:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_8:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_9:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_10:0' shape=(384, 4, 768) dtype=float32>, <tf.Tensor 'Placeholder_11:0' shape=(384, 4, 768) dtype=float32>]\n",
      "INFO:tensorflow:Use float type <dtype: 'float32'>\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jupyter/xlnet_pretrain/xlnet/modeling.py:535: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'model/transformer/assert_equal/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"xlnet/train_gpu.py\", line 331, in <module>\n",
      "    tf.app.run()  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
      "    _sys.exit(main(argv))  File \"xlnet/train_gpu.py\", line 327, in main\n",
      "    train(\"/gpu:0\")  File \"xlnet/train_gpu.py\", line 244, in train\n",
      "    mems=mems_i)  File \"xlnet/train_gpu.py\", line 164, in single_core_graph\n",
      "    is_training=is_training)  File \"xlnet/train_gpu.py\", line 140, in model_fn\n",
      "    FLAGS, features, labels, mems, is_training)  File \"/home/jupyter/xlnet_pretrain/xlnet/function_builder.py\", line 130, in get_loss\n",
      "    return two_stream_loss(FLAGS, features, labels, mems, is_training)  File \"/home/jupyter/xlnet_pretrain/xlnet/function_builder.py\", line 90, in two_stream_loss\n",
      "    inp_q=inp_q)  File \"/home/jupyter/xlnet_pretrain/xlnet/xlnet.py\", line 222, in __init__\n",
      "    ) = modeling.transformer_xl(**tfm_args)  File \"/home/jupyter/xlnet_pretrain/xlnet/modeling.py\", line 567, in transformer_xl\n",
      "    bsz=bsz, dtype=tf_float)  File \"/home/jupyter/xlnet_pretrain/xlnet/modeling.py\", line 236, in relative_positional_encoding\n",
      "    tf.debugging.assert_equal(bsz % 2, 0)  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py\", line 512, in assert_equal\n",
      "    return control_flow_ops.Assert(condition, data, summarize=summarize)  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "==================================\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/xlnet_pretrain/xlnet/modeling.py:67: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "INFO:tensorflow:#params: 116750336\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "2020-12-02 02:15:52.448777: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-12-02 02:15:52.454878: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-12-02 02:15:52.455600: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55642cf1d970 executing computations on platform Host. Devices:\n",
      "2020-12-02 02:15:52.455633: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2020-12-02 02:15:52.567456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-12-02 02:15:52.568037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 15.75GiB freeMemory: 14.55GiB\n",
      "2020-12-02 02:15:52.568077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2020-12-02 02:15:52.569298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-12-02 02:15:52.569334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2020-12-02 02:15:52.569344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2020-12-02 02:15:52.569736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14153 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
      "2020-12-02 02:15:52.572551: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5564289d7fe0 executing computations on platform CUDA. Devices:\n",
      "2020-12-02 02:15:52.572586: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2020-12-02 02:16:05.345309: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "INFO:tensorflow:[2000] | gnorm 1.19 lr 0.000297 | loss 5.51 | pplx  246.51, bpc  7.9455\n",
      "INFO:tensorflow:[3000] | gnorm 1.31 lr 0.000296 | loss 5.49 | pplx  241.13, bpc  7.9137\n",
      "INFO:tensorflow:[4000] | gnorm 1.12 lr 0.000294 | loss 5.48 | pplx  239.37, bpc  7.9031\n",
      "INFO:tensorflow:[5000] | gnorm 0.93 lr 0.000293 | loss 5.46 | pplx  236.24, bpc  7.8841\n",
      "INFO:tensorflow:[6000] | gnorm 0.82 lr 0.000291 | loss 5.48 | pplx  240.31, bpc  7.9088\n",
      "INFO:tensorflow:[7000] | gnorm 0.67 lr 0.000290 | loss 5.47 | pplx  238.61, bpc  7.8985\n",
      "INFO:tensorflow:[8000] | gnorm 0.67 lr 0.000288 | loss 5.45 | pplx  233.65, bpc  7.8682\n",
      "INFO:tensorflow:[9000] | gnorm 0.72 lr 0.000287 | loss 5.52 | pplx  248.75, bpc  7.9585\n",
      "INFO:tensorflow:[10000] | gnorm 0.69 lr 0.000285 | loss 5.48 | pplx  240.23, bpc  7.9083\n",
      "INFO:tensorflow:Model saved in path: ./dialpad_xlnet_32000/model/model.ckpt\n",
      "INFO:tensorflow:[11000] | gnorm 0.62 lr 0.000284 | loss 5.46 | pplx  235.73, bpc  7.8810\n",
      "INFO:tensorflow:[12000] | gnorm 0.57 lr 0.000282 | loss 5.46 | pplx  235.88, bpc  7.8819\n",
      "INFO:tensorflow:[13000] | gnorm 0.61 lr 0.000281 | loss 5.47 | pplx  237.40, bpc  7.8912\n",
      "INFO:tensorflow:[14000] | gnorm 0.70 lr 0.000279 | loss 5.46 | pplx  235.37, bpc  7.8788\n",
      "INFO:tensorflow:[15000] | gnorm 0.49 lr 0.000278 | loss 5.46 | pplx  234.97, bpc  7.8763\n",
      "INFO:tensorflow:[16000] | gnorm 0.49 lr 0.000276 | loss 5.47 | pplx  236.40, bpc  7.8851\n",
      "INFO:tensorflow:[17000] | gnorm 0.40 lr 0.000275 | loss 5.47 | pplx  237.91, bpc  7.8943\n",
      "INFO:tensorflow:[18000] | gnorm 0.43 lr 0.000273 | loss 5.47 | pplx  237.11, bpc  7.8894\n",
      "INFO:tensorflow:[19000] | gnorm 0.41 lr 0.000272 | loss 5.45 | pplx  233.26, bpc  7.8658\n",
      "INFO:tensorflow:[20000] | gnorm 0.41 lr 0.000270 | loss 5.46 | pplx  234.00, bpc  7.8703\n",
      "INFO:tensorflow:Model saved in path: ./dialpad_xlnet_32000/model/model.ckpt\n",
      "INFO:tensorflow:[21000] | gnorm 0.43 lr 0.000269 | loss 5.47 | pplx  237.81, bpc  7.8937\n",
      "INFO:tensorflow:[22000] | gnorm 0.46 lr 0.000267 | loss 5.46 | pplx  235.18, bpc  7.8776\n",
      "INFO:tensorflow:[23000] | gnorm 0.37 lr 0.000266 | loss 5.48 | pplx  239.06, bpc  7.9013\n",
      "INFO:tensorflow:[24000] | gnorm 0.35 lr 0.000264 | loss 5.46 | pplx  236.06, bpc  7.8830\n",
      "INFO:tensorflow:[25000] | gnorm 0.57 lr 0.000263 | loss 5.46 | pplx  236.09, bpc  7.8832\n",
      "INFO:tensorflow:[26000] | gnorm 0.48 lr 0.000261 | loss 5.45 | pplx  233.89, bpc  7.8697\n",
      "INFO:tensorflow:[27000] | gnorm 0.24 lr 0.000260 | loss 5.47 | pplx  238.63, bpc  7.8987\n",
      "INFO:tensorflow:[28000] | gnorm 0.26 lr 0.000258 | loss 5.46 | pplx  235.07, bpc  7.8769\n",
      "INFO:tensorflow:[29000] | gnorm 0.42 lr 0.000257 | loss 5.45 | pplx  232.53, bpc  7.8613\n",
      "INFO:tensorflow:[31000] | gnorm 0.30 lr 0.000254 | loss 5.48 | pplx  238.82, bpc  7.8998\n",
      "INFO:tensorflow:[32000] | gnorm 0.29 lr 0.000252 | loss 5.46 | pplx  235.15, bpc  7.8774\n",
      "INFO:tensorflow:[33000] | gnorm 0.33 lr 0.000251 | loss 5.46 | pplx  234.55, bpc  7.8738\n",
      "INFO:tensorflow:[34000] | gnorm 0.74 lr 0.000249 | loss 5.46 | pplx  235.29, bpc  7.8783\n",
      "INFO:tensorflow:[35000] | gnorm 0.89 lr 0.000248 | loss 5.46 | pplx  233.97, bpc  7.8702\n",
      "INFO:tensorflow:[36000] | gnorm 0.21 lr 0.000246 | loss 5.46 | pplx  233.97, bpc  7.8702\n",
      "INFO:tensorflow:[37000] | gnorm 0.62 lr 0.000245 | loss 5.45 | pplx  233.70, bpc  7.8685\n",
      "INFO:tensorflow:[38000] | gnorm 0.40 lr 0.000243 | loss 5.47 | pplx  237.33, bpc  7.8907\n",
      "INFO:tensorflow:[39000] | gnorm 0.36 lr 0.000242 | loss 5.46 | pplx  234.36, bpc  7.8726\n",
      "INFO:tensorflow:[40000] | gnorm 0.18 lr 0.000240 | loss 5.45 | pplx  233.85, bpc  7.8695\n",
      "INFO:tensorflow:Model saved in path: ./dialpad_xlnet_32000/model/model.ckpt\n",
      "INFO:tensorflow:[41000] | gnorm 0.36 lr 0.000239 | loss 5.47 | pplx  236.40, bpc  7.8851\n",
      "INFO:tensorflow:[42000] | gnorm 0.38 lr 0.000237 | loss 5.47 | pplx  236.62, bpc  7.8865\n",
      "INFO:tensorflow:[43000] | gnorm 0.17 lr 0.000236 | loss 5.45 | pplx  232.25, bpc  7.8596\n",
      "INFO:tensorflow:[44000] | gnorm 0.34 lr 0.000234 | loss 5.46 | pplx  234.99, bpc  7.8765\n",
      "INFO:tensorflow:[45000] | gnorm 0.15 lr 0.000233 | loss 5.45 | pplx  233.68, bpc  7.8684\n",
      "INFO:tensorflow:[46000] | gnorm 0.44 lr 0.000231 | loss 5.47 | pplx  238.64, bpc  7.8987\n",
      "INFO:tensorflow:[47000] | gnorm 0.24 lr 0.000230 | loss 5.46 | pplx  234.32, bpc  7.8724\n",
      "INFO:tensorflow:[48000] | gnorm 0.34 lr 0.000228 | loss 5.45 | pplx  233.13, bpc  7.8650\n",
      "INFO:tensorflow:[49000] | gnorm 0.46 lr 0.000227 | loss 5.47 | pplx  237.80, bpc  7.8936\n",
      "INFO:tensorflow:[50000] | gnorm 0.22 lr 0.000225 | loss 5.46 | pplx  234.33, bpc  7.8724\n",
      "INFO:tensorflow:Model saved in path: ./dialpad_xlnet_32000/model/model.ckpt\n",
      "INFO:tensorflow:[51000] | gnorm 0.39 lr 0.000224 | loss 5.46 | pplx  234.57, bpc  7.8739\n",
      "INFO:tensorflow:[52000] | gnorm 0.26 lr 0.000222 | loss 5.45 | pplx  232.06, bpc  7.8583\n",
      "INFO:tensorflow:[53000] | gnorm 0.19 lr 0.000221 | loss 5.46 | pplx  235.32, bpc  7.8785\n",
      "INFO:tensorflow:[54000] | gnorm 0.24 lr 0.000219 | loss 5.46 | pplx  235.46, bpc  7.8794\n",
      "INFO:tensorflow:[55000] | gnorm 0.28 lr 0.000218 | loss 5.44 | pplx  231.45, bpc  7.8546\n",
      "INFO:tensorflow:[56000] | gnorm 0.69 lr 0.000216 | loss 5.46 | pplx  234.51, bpc  7.8735\n",
      "INFO:tensorflow:[57000] | gnorm 0.18 lr 0.000215 | loss 5.47 | pplx  237.17, bpc  7.8898\n",
      "INFO:tensorflow:[58000] | gnorm 0.14 lr 0.000213 | loss 5.45 | pplx  233.60, bpc  7.8679\n",
      "INFO:tensorflow:[59000] | gnorm 0.30 lr 0.000212 | loss 5.46 | pplx  234.71, bpc  7.8747\n",
      "INFO:tensorflow:[60000] | gnorm 0.89 lr 0.000210 | loss 5.46 | pplx  234.77, bpc  7.8751\n",
      "INFO:tensorflow:Model saved in path: ./dialpad_xlnet_32000/model/model.ckpt\n",
      "INFO:tensorflow:[61000] | gnorm 0.45 lr 0.000209 | loss 5.45 | pplx  233.52, bpc  7.8674\n",
      "INFO:tensorflow:[62000] | gnorm 0.19 lr 0.000207 | loss 5.46 | pplx  234.11, bpc  7.8710\n",
      "INFO:tensorflow:[63000] | gnorm 0.17 lr 0.000206 | loss 5.45 | pplx  233.39, bpc  7.8666\n",
      "INFO:tensorflow:[64000] | gnorm 0.16 lr 0.000204 | loss 5.47 | pplx  236.76, bpc  7.8873\n",
      "INFO:tensorflow:[65000] | gnorm 0.15 lr 0.000203 | loss 5.46 | pplx  235.01, bpc  7.8766\n",
      "INFO:tensorflow:[66000] | gnorm 0.52 lr 0.000201 | loss 5.45 | pplx  232.65, bpc  7.8620\n",
      "INFO:tensorflow:[67000] | gnorm 0.66 lr 0.000200 | loss 5.45 | pplx  231.87, bpc  7.8572\n",
      "INFO:tensorflow:[68000] | gnorm 0.16 lr 0.000198 | loss 5.46 | pplx  236.25, bpc  7.8842\n",
      "INFO:tensorflow:[69000] | gnorm 0.19 lr 0.000197 | loss 5.45 | pplx  233.83, bpc  7.8693\n",
      "INFO:tensorflow:[70000] | gnorm 0.24 lr 0.000195 | loss 5.47 | pplx  237.77, bpc  7.8934\n",
      "INFO:tensorflow:Model saved in path: ./dialpad_xlnet_32000/model/model.ckpt\n",
      "INFO:tensorflow:[71000] | gnorm 0.34 lr 0.000194 | loss 5.46 | pplx  234.97, bpc  7.8764\n",
      "INFO:tensorflow:[72000] | gnorm 0.17 lr 0.000192 | loss 5.46 | pplx  234.63, bpc  7.8742\n",
      "INFO:tensorflow:[73000] | gnorm 0.16 lr 0.000191 | loss 5.45 | pplx  232.65, bpc  7.8620\n",
      "INFO:tensorflow:[74000] | gnorm 0.46 lr 0.000189 | loss 5.47 | pplx  237.55, bpc  7.8921\n",
      "INFO:tensorflow:[75000] | gnorm 0.73 lr 0.000188 | loss 5.45 | pplx  233.32, bpc  7.8662\n",
      "INFO:tensorflow:[76000] | gnorm 0.29 lr 0.000186 | loss 5.45 | pplx  231.89, bpc  7.8573\n",
      "INFO:tensorflow:[77000] | gnorm 0.51 lr 0.000185 | loss 5.46 | pplx  234.01, bpc  7.8704\n",
      "INFO:tensorflow:[78000] | gnorm 0.20 lr 0.000183 | loss 5.47 | pplx  237.01, bpc  7.8888\n",
      "INFO:tensorflow:[79000] | gnorm 0.40 lr 0.000182 | loss 5.46 | pplx  234.04, bpc  7.8706\n",
      "INFO:tensorflow:[80000] | gnorm 0.26 lr 0.000180 | loss 5.45 | pplx  233.71, bpc  7.8686\n",
      "INFO:tensorflow:Model saved in path: ./dialpad_xlnet_32000/model/model.ckpt\n",
      "INFO:tensorflow:[81000] | gnorm 0.11 lr 0.000179 | loss 5.46 | pplx  234.87, bpc  7.8757\n",
      "INFO:tensorflow:[82000] | gnorm 0.28 lr 0.000177 | loss 5.45 | pplx  233.13, bpc  7.8650\n",
      "INFO:tensorflow:[83000] | gnorm 0.26 lr 0.000176 | loss 5.45 | pplx  232.23, bpc  7.8594\n",
      "INFO:tensorflow:[84000] | gnorm 0.31 lr 0.000174 | loss 5.45 | pplx  232.63, bpc  7.8619\n",
      "INFO:tensorflow:[85000] | gnorm 0.30 lr 0.000173 | loss 5.46 | pplx  235.64, bpc  7.8805\n",
      "INFO:tensorflow:[86000] | gnorm 0.37 lr 0.000171 | loss 5.46 | pplx  234.27, bpc  7.8720\n",
      "INFO:tensorflow:[87000] | gnorm 0.22 lr 0.000170 | loss 5.45 | pplx  232.07, bpc  7.8584\n",
      "INFO:tensorflow:[88000] | gnorm 0.73 lr 0.000168 | loss 5.46 | pplx  235.85, bpc  7.8817\n",
      "INFO:tensorflow:[89000] | gnorm 0.71 lr 0.000167 | loss 5.46 | pplx  234.47, bpc  7.8733\n",
      "INFO:tensorflow:[90000] | gnorm 0.38 lr 0.000165 | loss 5.45 | pplx  231.62, bpc  7.8556\n",
      "INFO:tensorflow:Model saved in path: ./dialpad_xlnet_32000/model/model.ckpt\n",
      "INFO:tensorflow:[91000] | gnorm 0.17 lr 0.000164 | loss 5.45 | pplx  233.18, bpc  7.8653\n",
      "INFO:tensorflow:[92000] | gnorm 0.11 lr 0.000162 | loss 5.46 | pplx  234.28, bpc  7.8721\n",
      "INFO:tensorflow:[93000] | gnorm 0.73 lr 0.000161 | loss 5.46 | pplx  236.06, bpc  7.8830\n",
      "INFO:tensorflow:[94000] | gnorm 0.28 lr 0.000159 | loss 5.46 | pplx  234.41, bpc  7.8729\n",
      "INFO:tensorflow:[95000] | gnorm 1.22 lr 0.000158 | loss 5.45 | pplx  232.13, bpc  7.8588\n",
      "INFO:tensorflow:[96000] | gnorm 0.50 lr 0.000156 | loss 5.46 | pplx  235.17, bpc  7.8776\n",
      "INFO:tensorflow:[97000] | gnorm 1.02 lr 0.000155 | loss 5.45 | pplx  233.78, bpc  7.8690\n",
      "INFO:tensorflow:[98000] | gnorm 0.39 lr 0.000153 | loss 5.45 | pplx  233.85, bpc  7.8695\n"
     ]
    }
   ],
   "source": [
    "#Though the output is frozen at 98000 training steps, the latest model is savead at checkpoint:\n",
    "#100,000\n",
    "#110,000\n",
    "#\n",
    "\n",
    "!python xlnet/train_gpu.py --corpus_info_path=\"./dialpad_xlnet_32000/corpus_info.json\" --model_dir=\"./dialpad_xlnet_32000/model/\" --record_info_dir=\"./dialpad_xlnet_32000/tfrecords/\" --use_tpu=False --train_batch_size=4 --num_core_per_host=1 --vocab_size=32000 --seq_len=512 --reuse_len=256 --mem_len=384 --perm_size=256 --untie_r=True --mask_alpha=6 --mask_beta=1 --num_predict=85 --train_steps=200000 --iterations=1000 --save_steps=10000 --uncased=True --bi_data=True --d_head=64 --d_inner=3072 --d_model=768 --ff_activation=gelu --n_head=12 --n_layer=12 --learning_rate=3e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pretrained XLNet model from TF to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/xlnet_pretrain\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/home/jupyter/xlnet_pretrain/dialpad_xlnet_bi4/model/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFXLNetForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0092d092c458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFXLNetForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jupyter/xlnet_pretrain/dialpad_xlnet_bi4/model/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/dummy_tf_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         \u001b[0mrequires_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mrequires_tf\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTENSORFLOW_IMPORT_ERROR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nTFXLNetForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "model = TFXLNetForSequenceClassification.from_pretrained('/home/jupyter/xlnet_pretrain/dialpad_xlnet_bi4/model/', from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Loading TF weight beta1_power with shape []\n",
      "Loading TF weight beta2_power with shape []\n",
      "Loading TF weight global_step with shape []\n",
      "Loading TF weight model/lm_loss/bias with shape [16000]\n",
      "Loading TF weight model/lm_loss/bias/Adam with shape [16000]\n",
      "Loading TF weight model/lm_loss/bias/Adam_1 with shape [16000]\n",
      "Loading TF weight model/transformer/layer_0/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_0/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_0/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_1/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_1/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_10/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_10/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_11/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_11/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_2/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_2/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_3/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_3/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_4/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_4/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_5/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_5/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_6/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_6/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_7/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_7/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_8/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_8/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/ff/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_1/bias with shape [3072]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_1/bias/Adam with shape [3072]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_1/bias/Adam_1 with shape [3072]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_1/kernel with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_1/kernel/Adam with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_1/kernel/Adam_1 with shape [768, 3072]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_2/bias with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_2/bias/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_2/bias/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_2/kernel with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_2/kernel/Adam with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_9/ff/layer_2/kernel/Adam_1 with shape [3072, 768]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/LayerNorm/beta with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/LayerNorm/beta/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/LayerNorm/beta/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/LayerNorm/gamma/Adam with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/LayerNorm/gamma/Adam_1 with shape [768]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/k/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/k/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/k/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/o/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/o/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/o/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/q/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/q/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/q/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/r/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/r/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/r/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/v/kernel with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/v/kernel/Adam with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/layer_9/rel_attn/v/kernel/Adam_1 with shape [768, 12, 64]\n",
      "Loading TF weight model/transformer/mask_emb/mask_emb with shape [1, 1, 768]\n",
      "Loading TF weight model/transformer/mask_emb/mask_emb/Adam with shape [1, 1, 768]\n",
      "Loading TF weight model/transformer/mask_emb/mask_emb/Adam_1 with shape [1, 1, 768]\n",
      "Loading TF weight model/transformer/r_r_bias with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/r_r_bias/Adam with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/r_r_bias/Adam_1 with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/r_s_bias with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/r_s_bias/Adam with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/r_s_bias/Adam_1 with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/r_w_bias with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/r_w_bias/Adam with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/r_w_bias/Adam_1 with shape [12, 12, 64]\n",
      "Loading TF weight model/transformer/seg_embed with shape [12, 2, 12, 64]\n",
      "Loading TF weight model/transformer/seg_embed/Adam with shape [12, 2, 12, 64]\n",
      "Loading TF weight model/transformer/seg_embed/Adam_1 with shape [12, 2, 12, 64]\n",
      "Loading TF weight model/transformer/word_embedding/lookup_table with shape [16000, 768]\n",
      "Loading TF weight model/transformer/word_embedding/lookup_table/Adam with shape [16000, 768]\n",
      "Loading TF weight model/transformer/word_embedding/lookup_table/Adam_1 with shape [16000, 768]\n",
      "Importing model/lm_loss/bias\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/transformers-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/commands/transformers_cli.py\", line 35, in main\n",
      "    service.run()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/commands/convert.py\", line 142, in run\n",
      "    self._tf_checkpoint, self._config, self._pytorch_dump_output, self._finetuning_task_name\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/models/xlnet/convert_xlnet_original_tf_checkpoint_to_pytorch.py\", line 70, in convert_xlnet_checkpoint_to_pytorch\n",
      "    load_tf_weights_in_xlnet(model, config, tf_checkpoint_path)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/models/xlnet/modeling_xlnet.py\", line 196, in load_tf_weights_in_xlnet\n",
      "    ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n",
      "AssertionError: ('Pointer shape torch.Size([32000]) and array shape (16000,) mismatched', torch.Size([32000]), (16000,))\n"
     ]
    }
   ],
   "source": [
    "# export TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint\n",
    "# export TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config\n",
    "\n",
    "!transformers-cli convert --model_type xlnet --tf_checkpoint \"/home/jupyter/xlnet_pretrain/dialpad_xlnet_bi4/model/model.ckpt\" --config \"/home/jupyter/xlnet_pretrain/dialpad_xlnet_bi4/model/config.json\" --pytorch_dump_output \"/home/jupyter/xlnet_pretrain/dialpad_xlnet_bi4/pytorch_model\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu100.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu100:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
